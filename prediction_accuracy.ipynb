{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When done, write a script to count the fraction of ham and spam predictions.  Compare to the fraction in the Minio logs â€“ do they agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from flask import Flask, request, jsonify\n",
    "from botocore.errorfactory import ClientError\n",
    "import structlog  # for event logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the environment files\n",
    "load_dotenv()\n",
    "s3_resource = boto3.resource('s3',\n",
    "                             endpoint_url=os.getenv('ENDPOINT_URL'),\n",
    "                             aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                             aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                             aws_session_token=None,\n",
    "                             config=boto3.session.Config(signature_version='s3v4'),\n",
    "                             verify=False\n",
    "                             )\n",
    "s3_client = boto3.client('s3',\n",
    "                             endpoint_url=os.getenv('ENDPOINT_URL'),\n",
    "                             aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                             aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                             aws_session_token=None,\n",
    "                             config=boto3.session.Config(signature_version='s3v4'),\n",
    "                             verify=False\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3_bucket(file_name: str, bucket_name=\"prediction-log-files\"):\n",
    "    # read all the files in the bucket\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    found = False\n",
    "    data = \"\"\n",
    "    for obj in bucket.objects.all():\n",
    "        key = obj.key\n",
    "        if file_name in str(key):\n",
    "            body = obj.get()['Body'].read()\n",
    "            \n",
    "            if isinstance(body, bytes):\n",
    "                body = body.decode()\n",
    "            elif not isinstance(body, str):\n",
    "                body = str(body)\n",
    "                \n",
    "            print(\"%s : length = %s\" % (key, len(body)))\n",
    "                \n",
    "            data += body\n",
    "            # data = body\n",
    "            # data.append(body)\n",
    "            found = True\n",
    "    if found:\n",
    "        return data\n",
    "    else:\n",
    "        print(\"ERROR KEY NOT FOUND\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_file_20230203-155922.json : length = 16648\n",
      "log_file_20230203-160023.json : length = 55461\n",
      "log_file_20230203-160124.json : length = 55520\n",
      "log_file_20230203-160224.json : length = 55880\n",
      "log_file_20230203-160325.json : length = 55643\n",
      "log_file_20230203-160425.json : length = 55637\n",
      "log_file_20230203-160526.json : length = 55712\n",
      "log_file_20230203-160626.json : length = 55063\n",
      "log_file_20230203-160727.json : length = 54372\n",
      "log_file_20230203-160827.json : length = 54524\n",
      "log_file_20230203-160928.json : length = 54008\n",
      "log_file_20230203-161028.json : length = 54875\n",
      "log_file_20230203-161129.json : length = 54534\n",
      "log_file_20230203-161229.json : length = 51869\n",
      "log_file_20230203-161330.json : length = 46333\n",
      "log_file_20230203-161430.json : length = 53554\n",
      "log_file_20230203-161531.json : length = 54479\n",
      "log_file_20230203-161631.json : length = 48506\n",
      "log_file_20230203-161732.json : length = 44613\n",
      "log_file_20230203-161833.json : length = 53580\n",
      "log_file_20230203-161933.json : length = 54117\n",
      "log_file_20230203-162034.json : length = 45103\n",
      "log_file_20230203-162134.json : length = 17183\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True\n",
    "# read in all the data from S3 as one big string\n",
    "data = read_from_s3_bucket(file_name=\"log_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{\"predicted_class\": \"spam\", \"event\": \"classify_email:predicted_class\", \"timestamp\": \"2023-02-03T21:59:04.887237Z\"}\n",
      "Type: <class 'list'> of type: <class 'dict'>, Len 10000\n",
      "{\"predicted_class\": \"spam\", \"event\": \"classify_email:predicted_class\", \"timestamp\": \"2023-02-03T21:59:04.887237Z\"}\n"
     ]
    }
   ],
   "source": [
    "# Split the string of data into individual lines. Only works because backslash is escaped in string.\n",
    "data = data.split('\\n')\n",
    "data = data[0:-1]\n",
    "if DEBUG: print(len(data))\n",
    "if DEBUG: print(data[0])\n",
    "\n",
    "# convert every entry in the list to a JSON object. \n",
    "records = []\n",
    "for i in range(len(data)):\n",
    "    record = json.loads(data[i])\n",
    "    records.append(record)\n",
    "\n",
    "if DEBUG: print(\"Type: %s of type: %s, Len %d\" % (type(records), type(records[0]), len(records)))\n",
    "if DEBUG: print(json.dumps(records[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype              \n",
      "---  ------           --------------  -----              \n",
      " 0   predicted_class  10000 non-null  category           \n",
      " 1   event            10000 non-null  category           \n",
      " 2   timestamp        10000 non-null  datetime64[ns, UTC]\n",
      "dtypes: category(2), datetime64[ns, UTC](1)\n",
      "memory usage: 176.0 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load the data into a dataframe and set proper types\n",
    "df = pd.DataFrame(records)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "df['predicted_class'] = df['predicted_class'].astype('category')\n",
    "df['event'] = df['event'].astype('category')\n",
    "df = df.sort_values(by='timestamp')\n",
    "\n",
    "if DEBUG: print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ham: 2786\tn_spam: 7214\n"
     ]
    }
   ],
   "source": [
    "ham = df[df['predicted_class']=='ham']\n",
    "spam = df[df['predicted_class']=='spam']\n",
    "n_ham = len(ham)\n",
    "n_spam = len(spam)\n",
    "print(\"n_ham: %d\\tn_spam: %d\" % (n_ham, n_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The logs predict 38.62 percent ham\n"
     ]
    }
   ],
   "source": [
    "pct_ham = n_ham / n_spam * 100\n",
    "print(\"The logs predict %.2f percent ham\" % pct_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
